{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing relevant modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import re\n",
    "import math\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the url for the review data\n",
    "url1 = \"https://raw.githubusercontent.com/cbannard/lela60331_24-25/refs/heads/main/coursework/Compiled_Reviews.txt\"\n",
    "urllib.request.urlretrieve(url1, \"Compiled_Reviews.txt\")\n",
    "\n",
    "#creating lists for the review data to go into\n",
    "reviews=[]\n",
    "sentiment_ratings=[]\n",
    "product_types=[]\n",
    "helpfulness_ratings=[]\n",
    "\n",
    "#splitting the data into relevant lists\n",
    "with open(\"Compiled_Reviews.txt\", encoding=\"utf-8\") as f:\n",
    "   for line in f.readlines()[1:]:\n",
    "        fields = line.rstrip().split('\\t')\n",
    "        reviews.append(fields[0])\n",
    "        sentiment_ratings.append(fields[1])\n",
    "        product_types.append(fields[2])\n",
    "        helpfulness_ratings.append(fields[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenising the sentences in reviews\n",
    "tokenised_sentences = [re.findall(\"[^ ]+\", text) for text in reviews]\n",
    "\n",
    "#Creating a token list from the tokenised sentences\n",
    "tokens=[]\n",
    "for s in tokenised_sentences:\n",
    "  tokens.extend(s)\n",
    "\n",
    "#counting the tokens\n",
    "counts = Counter(tokens)\n",
    "\n",
    "#sorts tokens with their frequencies and puts them in a descending order\n",
    "so=sorted(counts.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "#creates a list of only the tokens\n",
    "so=list(zip(*so))[0]\n",
    "\n",
    "#creates a vocabulary of the 5000 most frequent words \n",
    "type_list=so[0:5000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding the words\n",
    "\n",
    "#creates matrix of reviews the size of the reviews by the vocab \n",
    "M = np.zeros((len(reviews), len(type_list)))\n",
    "\n",
    "#iterates through the reviews and checks if the word appears in appears in the vocab. then indicates it with a 1 if its there.\n",
    "for i, rev in enumerate(reviews):\n",
    "  tokens = re.findall(\"[^ ]+\",rev)\n",
    "  for j,t in enumerate(type_list):\n",
    "    if t in tokens:\n",
    "      M[i,j] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the data into sets \n",
    "\n",
    "#Randomly seperating the data - 80% for the training set and 20% for the testing set and not allowing repeats\n",
    "train = np.random.choice(len(reviews), int(len(reviews)*0.8), replace=False)\n",
    "test = list(set(range(0,len(reviews))) - set(train))\n",
    "\n",
    "#creating matrices based on the training and testing sets\n",
    "M_train = M[train,]\n",
    "M_test = M[test,]\n",
    "\n",
    "#creating labels for the training and test set.\n",
    "SR_train = [sentiment_ratings[i] for i in train]\n",
    "SR_test = [sentiment_ratings[i] for i in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic regression\n",
    "\n",
    "#the input to the model is equal to the vocab size\n",
    "num_features=5000\n",
    "#encodes the sentiment labels. 1 for pos and 0 for neg\n",
    "y=[int(1==\"positive\") for l in SR_train]\n",
    "#initiliases the weights\n",
    "weights = np.random.rand(num_features)\n",
    "#initialises the bias\n",
    "bias=np.random.rand(1)\n",
    "#sets the number of iterations and the learning rate\n",
    "n_iters = 1000\n",
    "lr = 0.4\n",
    "#creates an empty list for the loss\n",
    "logistic_loss = []\n",
    "#sets the total number of labels\n",
    "num_samples = len(y)\n",
    "\n",
    "for i in range(n_iters):\n",
    "  #forward pass\n",
    "  z = M_train.dot(weights)+bias\n",
    "  #applies the sigmoid function\n",
    "  q = 1/(1+np.exp(-z))\n",
    "  #creating a small constant to avoid division byy 0\n",
    "  eps = 0.00001\n",
    "  #calculating loss and appending it to the list\n",
    "  loss = -sum((y*np.log2(q+eps)+(np.ones(len(y))-y)*np.log2(np.ones(len(y))-q+eps)))\n",
    "  logistic_loss.append(loss)\n",
    "  #turns predictions into 1 or 0\n",
    "  y_pred = [int(ql > 0.5) for ql in q]\n",
    "\n",
    "  #calculating the gradient w respect to the weights and the bias\n",
    "  dw = (q-y).dot(M_train)/num_samples\n",
    "  db = (sum(q-y))/num_samples\n",
    "\n",
    "#updating the weights and bias values\n",
    "  weights = weights - lr*dw\n",
    "  bias = bias - lr*db\n",
    "\n",
    "#plotting the loss\n",
    "plt.plot(range(1,n_iters), logistic_loss[1:])\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of probabilities\n",
    "\n",
    "#calculates the values for the test data\n",
    "z = M_test.dot(weights)+bias\n",
    "#converts values in z to probabilities \n",
    "q = 1/(1+np.exp(-z))\n",
    "#creates empty list for the test set predictions\n",
    "y_test_pred = []\n",
    "\n",
    "#appends the test predicitons and makes them binary outputs. 1 for pos and 0 for neg\n",
    "for prob in q:\n",
    "    if prob > 0.5:\n",
    "      y_test_pred.append(1)\n",
    "    else:\n",
    "      y_test_pred.append(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating accuracy\n",
    "\n",
    "#the true values of the reviews\n",
    "y_test=[int(l == \"positive\") for l in SR_test]\n",
    "#checks is predicted label matches true label\n",
    "acc_test=[int(yp == y_test[s]) for s,yp in enumerate(y_test_pred)]\n",
    "#calculates accuracy\n",
    "Accuracy = (sum(acc_test)/len(acc_test))\n",
    "\n",
    "print(f'Accuracy: {Accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating precision and recall\n",
    "\n",
    "#the predictions of sentiment rating in the test set \n",
    "SR_test_pred=[\"positive\" if s == 1 else \"negative\" for s in y_test_pred]\n",
    "\n",
    "#calculating the true pos, false pos, true neg and false neg predictions\n",
    "true_positives=sum([int(yp == \"positive\" and SR_test[s] == \"positive\") for s,yp in enumerate(SR_test_pred)])\n",
    "true_negatives=sum([int(yp == \"negative\" and SR_test[s] == \"negative\") for s,yp in enumerate(SR_test_pred)])\n",
    "false_positives=sum([int(yp == \"positive\" and SR_test[s] == \"negative\") for s,yp in enumerate(SR_test_pred)])\n",
    "false_negatives=sum([int(yp == \"negative\" and SR_test[s] == \"positive\") for s,yp in enumerate(SR_test_pred)])\n",
    "\n",
    "#calculating precision and recall\n",
    "precision = true_positives/(true_positives+false_positives)\n",
    "recall = true_positives/(true_positives+false_negatives)\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#top 10 words for a negative outcome\n",
    "neg_weights = [type_list[x] for x in np.argsort(weights)[0:10]]\n",
    "\n",
    "#top 10 words weighted for a positive outcome\n",
    "pos_weights = [type_list[x] for x in np.argsort(weights)[::-1][0:10]]\n",
    "\n",
    "print(f'The top 10 negative weighted words are {neg_weights} and the top 10 of positively weighted words are {pos_weights}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
