{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing relevant modules\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import re\n",
    "import math\n",
    "import urllib.request\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the url for the review data\n",
    "url1 = \"https://raw.githubusercontent.com/cbannard/lela60331_24-25/refs/heads/main/coursework/Compiled_Reviews.txt\"\n",
    "urllib.request.urlretrieve(url1, \"Compiled_Reviews.txt\")\n",
    "\n",
    "#creating lists for the review data to go into\n",
    "reviews=[]\n",
    "sentiment_ratings=[]\n",
    "product_types=[]\n",
    "helpfulness_ratings=[]\n",
    "\n",
    "#splitting the data into relevant lists\n",
    "with open(\"Compiled_Reviews.txt\", encoding=\"utf-8\") as f:\n",
    "   for line in f.readlines()[1:]:\n",
    "        fields = line.rstrip().split('\\t')\n",
    "        reviews.append(fields[0])\n",
    "        sentiment_ratings.append(fields[1])\n",
    "        product_types.append(fields[2])\n",
    "        helpfulness_ratings.append(fields[3])\n",
    "\n",
    "#making all the reviews lower case\n",
    "reviews = [review.lower() for review in reviews]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#downloading a list of stopwords from github\n",
    "url2 =  \"https://raw.githubusercontent.com/Alir3z4/stop-words/master/english.txt\"\n",
    "file_path = \"english.txt\"\n",
    "urllib.request.urlretrieve(url2, file_path)\n",
    "\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    stop_words = file.read().splitlines()\n",
    "\n",
    "#making the stop words lower case\n",
    "stop_words = [word.lower() for word in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenising and removing punctuation and stopwords\n",
    "tokenised_sentences = [\n",
    "    [word for word in re.findall(\"[^ \\.,?\\\":()-]+\", text) if word not in stop_words]\n",
    "    for text in reviews]\n",
    "\n",
    "#Creating a vocabulary \n",
    "\n",
    "#Creating a token list from the tokenised sentences\n",
    "tokens=[]\n",
    "for s in tokenised_sentences:\n",
    "  tokens.extend(s)\n",
    "\n",
    "#counting tokens\n",
    "counts = Counter(tokens)\n",
    "\n",
    "#sorts tokens with their frequencies and puts them in a descending order\n",
    "so=sorted(counts.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "#creates a list of only the tokens\n",
    "so=list(zip(*so))[0]\n",
    "\n",
    "#creates a vocabulary of the 5000 most frequent words \n",
    "type_list=so[0:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding the words\n",
    "\n",
    "#creates matrix of reviews the size of the reviews by the vocab \n",
    "M = np.zeros((len(reviews), len(type_list)))\n",
    "\n",
    "#iterates through the reviews and checks if the word appears in appears in the vocab. then indicates it with a 1 if its there.\n",
    "for i, rev in enumerate(reviews):\n",
    "  tokens = re.findall(\"[^ ]+\",rev)\n",
    "  for j,t in enumerate(type_list):\n",
    "    if t in tokens:\n",
    "      M[i,j] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the data into sets \n",
    "\n",
    "#Randomly seperating the data - 80% for the training set and 20% for the testing set and not allowing repeats\n",
    "train = np.random.choice(len(reviews), int(len(reviews)*0.8), replace=False)\n",
    "test = list(set(range(0,len(reviews))) - set(train))\n",
    "\n",
    "#creating matrices based on the training and testing sets\n",
    "M_train = M[train,]\n",
    "M_test = M[test,]\n",
    "\n",
    "#creating labels for the training and test set.\n",
    "SR_train = [sentiment_ratings[i] for i in train]\n",
    "SR_test = [sentiment_ratings[i] for i in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multilayer neural network\n",
    "\n",
    "#the input to the model is equal to the vocab size\n",
    "num_features=5000\n",
    "#number of iterations and the learning rate\n",
    "n_iters = 2500\n",
    "lr = 0.1\n",
    "#encodes the sentiment labels. 1 for pos and 0 for neg\n",
    "y=[1 if label == \"positive\" else 0 for label in SR_train]\n",
    "#sets the total number of labels\n",
    "num_samples = len(y)\n",
    "#creates an empty list for the loss\n",
    "logistic_loss = []\n",
    "\n",
    "#setting the number of nodes in the hidden layer\n",
    "hidden_layer = 3\n",
    "\n",
    "#initialising the layers of weights\n",
    "weights_0_1 = np.random.rand(num_features, hidden_layer)\n",
    "weights_1_2 = np.random.rand(hidden_layer, 1) \n",
    "\n",
    "#initilaisng the bias values\n",
    "bias_0_1 = np.zeros((1, hidden_layer))\n",
    "bias_1_2 = np.zeros((1, 1))\n",
    "\n",
    "\n",
    "\n",
    "for i in range(n_iters):\n",
    "\n",
    "  #forward pass\n",
    "  #hidden layer\n",
    "  z_1 = np.dot(M_train, weights_0_1) + bias_0_1\n",
    "  #ReLU activation \n",
    "  hidden_relu = np.maximum(0, z_1)\n",
    "\n",
    "  #output layer\n",
    "  z_2 = np.dot(hidden_relu, weights_1_2) + bias_1_2\n",
    "  #sigmoid activation\n",
    "  q = 1/(1 + np.exp(-z_2))\n",
    "\n",
    "  #loss\n",
    "  #small constant to avoid division by 0\n",
    "  eps = 0.00001\n",
    "  #shape of the true labels\n",
    "  y = np.array(y).flatten()\n",
    "  #shape of the predicted probabilities\n",
    "  q = q.flatten()\n",
    "  #loss\n",
    "  loss = -np.mean(y * np.log(q + eps) + (1 - y) * np.log(1 - q + eps))\n",
    "  logistic_loss.append(loss)\n",
    "\n",
    "  #backwards pass\n",
    "  #error at the output layer\n",
    "  error_output = (q - y).reshape(-1, 1)\n",
    "  #error of the hidden layer\n",
    "  error_hidden = np.dot(error_output, weights_1_2.T) * (z_1 > 0)\n",
    "\n",
    "  #calculating gradients for second layer weights and bias\n",
    "  dw_1_2 = np.dot(hidden_relu.T, error_output) / num_samples\n",
    "  db_1_2 = np.mean(error_output)\n",
    "\n",
    "  #calculating gradients for first layer weights and bias\n",
    "  dw_0_1 = np.dot(M_train.T, error_hidden) / num_samples\n",
    "  db_0_1 = np.mean(error_hidden)\n",
    "\n",
    "  #updating weights and biases\n",
    "\n",
    "  weights_1_2 -= lr * dw_1_2\n",
    "  bias_1_2 -= lr * db_1_2\n",
    "\n",
    "  weights_0_1 -= lr * dw_0_1\n",
    "  bias_0_1 -= lr * db_0_1\n",
    "\n",
    "  #converting the predicted probabilities to binary\n",
    "  y_pred = [int(ql > 0.5) for ql in q]\n",
    "\n",
    "\n",
    "#plotting loss gradient\n",
    "plt.plot(range(1,n_iters), logistic_loss[1:])\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing true values against predictions\n",
    "\n",
    "#output of the hidden layer with the test set\n",
    "layer_1 = np.maximum(M_test.dot(weights_0_1) + bias_0_1, 0) \n",
    "#output of the output layer with the test set\n",
    "layer_2 = layer_1.dot(weights_1_2) + bias_1_2\n",
    "#sigmoid activation\n",
    "q = 1/(1+np.exp(-layer_2))\n",
    "\n",
    "#convert predictions to binary\n",
    "y_test_pred = [int(prob > 0.5) for prob in q]\n",
    "print(y_test_pred)\n",
    "#create true value labels\n",
    "y_test=[int(l == \"positive\") for l in SR_test]\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing accuracy, precision, recall \n",
    "\n",
    "#list of correct predictions\n",
    "acc_test=[int(yp == y_test[s]) for s,yp in enumerate(y_test_pred)]\n",
    "#calculating accuracy\n",
    "print(f'accuracy: {sum(acc_test)/len(acc_test)}')\n",
    "\n",
    "\n",
    "labels_test_pred=[\"positive\" if s == 1 else \"negative\" for s in y_test_pred]\n",
    "\n",
    "#calculating true pos, true neg, false pos, false neg. \n",
    "true_positives=sum([int(yp == \"positive\" and SR_test[s] == \"positive\") for s,yp in enumerate(labels_test_pred)])\n",
    "true_negatives=sum([int(yp == \"negative\" and SR_test[s] == \"negative\") for s,yp in enumerate(labels_test_pred)])\n",
    "false_positives=sum([int(yp == \"positive\" and SR_test[s] == \"negative\") for s,yp in enumerate(labels_test_pred)])\n",
    "false_negatives=sum([int(yp == \"negative\" and SR_test[s] == \"positive\") for s,yp in enumerate(labels_test_pred)])\n",
    "\n",
    "\n",
    "#calculating precision and recall\n",
    "precision = true_positives/(true_positives + false_positives)\n",
    "recall = true_positives/(true_positives + false_negatives)\n",
    "print(f'precision: {precision}')\n",
    "print(f'recall: {recall}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identifying positive and negatively weighted tokens\n",
    "\n",
    "#combining all weights\n",
    "all_weights = np.dot(weights_0_1, weights_1_2).flatten()\n",
    "\n",
    "#sorting them by highest weighted\n",
    "postokens = [type_list[x] for x in np.argsort(all_weights)[::-1][:20]]\n",
    "negtokens = [type_list[x] for x in np.argsort(all_weights)[:20]]\n",
    "\n",
    "print(f'Positive tokens: {postokens} \n",
    "      Negative tokens: {negtokens}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
